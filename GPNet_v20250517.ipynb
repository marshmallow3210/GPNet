import tensorflow as tf
from tensorflow.keras import layers, Model, Input

# Feature Enhancement Module (FEM)
def FEM(x, reduction=16, name="FEM"):
    ch = x.shape[-1]
    dw    = layers.DepthwiseConv2D(3, padding='same', name=f'{name}_dw')(x)
    gap   = layers.GlobalAveragePooling2D(name=f'{name}_gap')(dw)
    fc1   = layers.Dense(ch // reduction, activation='relu', name=f'{name}_fc1')(gap)
    fc2   = layers.Dense(ch, activation='sigmoid', name=f'{name}_fc2')(fc1)
    scale = layers.Reshape((1, 1, ch), name=f'{name}_reshape')(fc2)
    exc   = layers.Multiply(name=f'{name}_scale')([x, scale])
    return layers.Add(name=f'{name}_add')([x, exc])

# Segmentation Module (SAM + CAM) 
def SegmentationModule(x, name="SM"):
    ch = x.shape[-1]           # e.g. 512
    H  = x.shape[1]            # 72
    W  = x.shape[2]            # 40
    c_v = ch // 4              # 128
    c_q = ch // 32             # 16

    # 1) 降維
    x_c = layers.Conv2D(c_v, 1, padding='same', name=f'{name}_reduce_conv')(x)
    x_c = layers.BatchNormalization(name=f'{name}_reduce_bn')(x_c)
    x_c = layers.ReLU(name=f'{name}_reduce_relu')(x_c)

    # 2) Spatial Attention (SAM)
    # Q, K, V 都來自 x_c
    q = layers.Conv2D(c_q, 1, padding='same', name=f'{name}_sam_q')(x_c)
    k = layers.Conv2D(c_q, 1, padding='same', name=f'{name}_sam_k')(x_c)
    v = layers.Conv2D(c_v, 1, padding='same', name=f'{name}_sam_v')(x_c)
    # reshape 成 (B, H*W, channels)
    qf = layers.Reshape((H*W, c_q), name=f'{name}_sam_qf')(q)
    kf = layers.Reshape((H*W, c_q), name=f'{name}_sam_kf')(k)
    vf = layers.Reshape((H*W, c_v), name=f'{name}_sam_vf')(v)
    # 注意力計算
    attn_s = layers.Lambda(lambda t: tf.matmul(t[0], t[1], transpose_b=True),
                           name=f'{name}_sam_mat')([qf, kf])      # (B, HW, HW)
    attn_s = layers.Softmax(axis=-1, name=f'{name}_sam_softmax')(attn_s)
    sa = layers.Lambda(lambda t: tf.matmul(t[0], t[1]),
                       name=f'{name}_sam_out_mat')([attn_s, vf])  # (B, HW, C')
    sa = layers.Reshape((H, W, c_v), name=f'{name}_sam_out')(sa)

    # 3) Channel Attention (CAM)
    # flatten spatial到 (B, HW, C')
    flat = layers.Reshape((H*W, c_v), name=f'{name}_cam_flat')(x_c)
    # 轉置到 (B, C', HW)
    flat_t = layers.Permute((2,1), name=f'{name}_cam_perm')(flat)
    attn_c = layers.Lambda(lambda t: tf.matmul(t[0], t[1], transpose_b=True),
                           name=f'{name}_cam_mat')([flat_t, flat_t])  # (B, C', C')
    attn_c = layers.Softmax(axis=-1, name=f'{name}_cam_softmax')(attn_c)
    ca_pre = layers.Lambda(lambda t: tf.matmul(t[0], t[1]),
                           name=f'{name}_cam_out_mat')([attn_c, flat_t])  # (B, C', HW)
    # 轉回 (B, HW, C') → 再 reshape 回 (B, H, W, C')
    flat_ca = layers.Permute((2,1), name=f'{name}_cam_unperm')(ca_pre)
    ca = layers.Reshape((H, W, c_v), name=f'{name}_cam_out')(flat_ca)

    # 4) SA + CA 融合 → Conv→BN→ReLU → 1x1 Conv + Sigmoid
    attn_map = layers.Add(name=f'{name}_add')([sa, ca])
    attn_map = layers.Conv2D(c_v, 1, padding='same', name=f'{name}_attn_conv1')(attn_map)
    attn_map = layers.BatchNormalization(name=f'{name}_attn_bn1')(attn_map)
    attn_map = layers.ReLU(name=f'{name}_attn_relu1')(attn_map)
    attn_map = layers.Conv2D(ch, 1, activation='sigmoid',
                             padding='same', name=f'{name}_attn_conv2')(attn_map)

    # 5) 乘回原始特徵
    return layers.Multiply(name=f'{name}_out')([x, attn_map])

# Density Map Estimator (DME)
def DensityHead(x, name="DME"):
    x = layers.Conv2D(256, 3, padding='same', activation='relu',
                      dilation_rate=2, name=f'{name}_c1')(x)
    x = layers.Conv2D(128, 3, padding='same', activation='relu',
                      dilation_rate=2, name=f'{name}_c2')(x)
    x = layers.Conv2D(64, 3, padding='same', activation='relu',
                      name=f'{name}_c3')(x)
    return layers.Conv2D(1, 1, padding='same', activation='linear',
                         name=f'{name}_out')(x)

def GPNet(H=576, W=320, C=3):
    inputs = Input(shape=(H, W, C), name='input')

    # FME Stage1
    x = layers.Conv2D(64, 3, activation='relu', padding='same')(inputs)
    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D(2, name='pool1')(x)  # → 288×160

    # FME Stage2
    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D(2, name='pool2')(x)  # → 144×80

    # Stage3 + FEM3
    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)
    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)
    x = layers.Conv2D(256, 3, activation='relu', padding='same')(x)
    x = layers.MaxPooling2D(2, name='pool3')(x)  # → 72×40
    x = FEM(x, name='FEM3')

    # Stage4 + FEM4
    x = layers.Conv2D(512, 3, activation='relu', padding='same')(x)
    x = layers.Conv2D(512, 3, activation='relu', padding='same')(x)
    x = layers.Conv2D(512, 3, activation='relu', padding='same')(x)
    x = FEM(x, name='FEM4')

    # Segmentation Module (Neck)
    x = SegmentationModule(x, name='SM')

    # Density Head (Output)
    outputs = DensityHead(x, name='DME')

    return Model(inputs, outputs, name='GPNet')

if __name__ == "__main__":
    model = GPNet(576, 320, 3)
    model.summary()
    print("Output shape:", model.output_shape)  # (None, 72, 40, 1)
